{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1596355,"sourceType":"datasetVersion","datasetId":941801}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:37:47.146168Z","iopub.execute_input":"2025-07-09T18:37:47.146431Z","iopub.status.idle":"2025-07-09T18:37:58.038654Z","shell.execute_reply.started":"2025-07-09T18:37:47.146402Z","shell.execute_reply":"2025-07-09T18:37:58.037751Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((img_width, img_height)),\n        transforms.ToTensor(),  # Convertir les images en tenseur PyTorch\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normaliser les images\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((img_width, img_height)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = \"/kaggle/input/airbus-wind-turbines-patches\"\n\ntrain_dataset = datasets.ImageFolder(data_dir + '/train', transform=data_transforms['train'])\nval_dataset   = datasets.ImageFolder(data_dir + '/validation', transform=data_transforms['val'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:37:58.040332Z","iopub.execute_input":"2025-07-09T18:37:58.040707Z","iopub.status.idle":"2025-07-09T18:37:58.065536Z","shell.execute_reply.started":"2025-07-09T18:37:58.040684Z","shell.execute_reply":"2025-07-09T18:37:58.064383Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3756694465.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/airbus-wind-turbines-patches\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/validation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_transforms' is not defined"],"ename":"NameError","evalue":"name 'data_transforms' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32\nlearning_rate = 0.001\nnum_epochs = 10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   # 1ère couche convolutionnelle\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # 2ème couche convolutionnelle\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # 3ème couche convolutionnelle\n        self.pool = nn.MaxPool2d(2, 2)                            # Max pooling avec kernel de 2\n        self.fc1 = nn.Linear(128 * 16 * 16, 256)                  # 1ère couche complètement connectée\n        self.fc2 = nn.Linear(256, 2)                              # 2ème couche complètement connectée (sortie)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))  # Convolution + ReLU + Pooling\n        x = self.pool(torch.relu(self.conv2(x)))  # Convolution + ReLU + Pooling\n        x = self.pool(torch.relu(self.conv3(x)))  # Convolution + ReLU + Pooling\n        x = nn.Flatten()(x)                       # Aplatir les données\n        x = torch.relu(self.fc1(x))               # Couche FC + ReLU\n        x = self.fc2(x)                           # Couche FC finale\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialiser le modèle\nmodel = CNN()\n\n# Déplacer le modèle sur GPU si disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Charger les données\ntrain_dataset, val_dataset = get_data(data_dir, data_transforms)\n\n# Créer les DataLoaders\ntrain_loader, val_loader = get_loaders(train_dataset, val_dataset, batch_size)\n\n# Définir la Fonction de Coût et l’Optimiseur\ncriterion = nn.CrossEntropyLoss() # Choisir une fonction de perte (par exemple, entropie croisée)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate) # Choisir un optimiseur (par exemple, Adam ou SGD)\n\n# Entraîner le modèle\ntrain_model(model, device, train_loader, val_loader, optimizer, criterion, num_epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Déplacer le modèle sur GPU si disponible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Charger les données de test\nprediction_dir = './data/All/test/'\ntest_dataset = get_test_data(prediction_dir, data_transforms)\n\n# Créer le DataLoader pour le test\ntest_loader = get_test_loader(test_dataset, 1)\n\n# Tester le modèle\nresults_df = test_model(model, device, test_dataset, test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}